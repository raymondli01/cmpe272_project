# AI-Driven Data Implementation Guide

## Step 1: Run the SQL Migration

Go to your Supabase Dashboard → SQL Editor and run the entire contents of:
`supabase/migrations/20251113000001_add_ai_analytics_tables.sql`

This will create 4 new tables:
- `ai_analytics` - Stores dashboard metrics (NRW, uptime, etc.)
- `energy_schedules` - Stores AI-generated pump schedules
- `demand_forecasts` - Stores 24-hour water demand predictions
- `system_metrics` - Stores system performance metrics

## Step 2: Generate Initial AI Data

After running the migration, you need to populate the tables with AI-generated data.

I'll add an API endpoint `/ai/generate-analytics` that you can call to populate all the data.

Run this once to generate initial data:
```bash
curl -X POST http://localhost:8000/ai/generate-analytics
```

This will:
1. Generate NRW calculations
2. Calculate system uptime
3. Create 24-hour demand forecast
4. Generate energy optimization schedules
5. Store everything in the database

## Step 3: Frontend Changes

The frontend pages will be updated to fetch data from:
- Dashboard: `ai_analytics`, `demand_forecasts`
- Energy: `energy_schedules`, `ai_analytics`
- All pages: Remove hardcoded values

## What's Being Automated

### Dashboard.tsx
**Before (Hardcoded):**
- Non-Revenue Water: 12.4%
- Energy Cost: $287
- Network Uptime: 99.7%
- Demand Forecast: Random mock data

**After (AI-Generated):**
- NRW: Calculated from leak events + flow sensors
- Energy Cost: From latest energy_schedules table
- Uptime: Calculated from events + sensor availability
- Demand Forecast: AI prediction from demand_forecasts table

### Energy.tsx
**Before (Hardcoded):**
- Today's Savings: $142.50
- Efficiency Gain: 18.5%
- Pump Schedule: 4 hardcoded time slots

**After (AI-Generated):**
- All metrics from `energy_schedules` table
- Pump schedules generated by Energy Optimizer Agent
- Real-time updates when agents run

## How Often Will AI Generate Data?

Option 1: **On-Demand** - Call `/ai/generate-analytics` whenever you want fresh data
Option 2: **Scheduled** - Set up a cron job to run daily
Option 3: **Event-Driven** - Generate analytics when new events are created

I'm implementing Option 1 first, then we can add scheduling later.

## Files Created/Modified

### Backend
- ✅ `backend/ai_agents/analytics_agent.py` - New analytics agent
- ✅ `backend/ai_agents/energy_optimizer_agent.py` - Updated to store schedules
- ⏳ `backend/main.py` - Add `/ai/generate-analytics` endpoint (next step)

### Frontend
- ⏳ `frontend/src/pages/Dashboard.tsx` - Remove hardcoded values
- ⏳ `frontend/src/pages/Energy.tsx` - Fetch from energy_schedules

### Database
- ✅ Migration file created: `supabase/migrations/20251113000001_add_ai_analytics_tables.sql`

## Next Steps (What I'm doing now)

1. ✅ Create migration SQL
2. ✅ Create Analytics Agent
3. ✅ Update Energy Optimizer to store schedules
4. ⏳ Add API endpoint for analytics generation
5. ⏳ Update Dashboard.tsx to fetch AI data
6. ⏳ Update Energy.tsx to fetch AI data
7. ⏳ Test everything works

## Testing the System

After everything is set up:

1. Run the migration in Supabase
2. Start your backend: `./start-services.sh`
3. Generate analytics: `curl -X POST http://localhost:8000/ai/generate-analytics`
4. Refresh the dashboard - you should see AI-generated data!
5. Run Energy Optimizer: Click "Run Analysis" on Energy Optimizer Agent
6. Refresh Energy page - you should see the AI-generated schedule!

## Cost Estimates

Each time you generate analytics (call the endpoint):
- NRW Calculation: ~$0.01
- Uptime Calculation: ~$0.01
- Demand Forecast: ~$0.02
- Total: ~$0.04 per generation

Running once per day = ~$1.20/month
Running every hour = ~$28.80/month

Recommend: Run once daily at midnight, or on-demand when needed.
